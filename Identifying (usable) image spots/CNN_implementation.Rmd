---
title: "ST_pipeline"
output: html_document
---
Data: https://support.10xgenomics.com/spatial-gene-expression/datasets/1.0.0/V1_Breast_Cancer_Block_A_Section_1?
File contents help: https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/images
NN tutorial: https://blog.rstudio.com/2017/09/05/keras-for-r/


You MUST first unpack the gz files!!
```{r setup, include=FALSE}
#devtools::install_github("satijalab/seurat", ref = "spatial")
library(backports)
library(Seurat)
#library(SeuratData)
library(ggplot2)
library(patchwork)
library(dplyr)


library(Matrix)
library(rjson)
library(cowplot)
library(RColorBrewer)
library(grid)
library(readbitmap)
library(hdf5r)
library(data.table)


library(raster)


library(keras)
library(EBImage)
library(stringr)
library(pbapply)
```

Loading in spot position csv
```{r}
# Load in spot location CSV file(doesn't have headers)
image_location <- read.csv("data/spatial/tissue_positions_list.csv", header=FALSE)
names(image_location) <- c("barcode", "in_tissue", "array_row", "array_col", "pxl_col_in_fullres_yValue", "pxl_row_in_fullres_xValue")
```
 
Loading in the Whole Slide Image (WSI)
```{r}
file_name <- 'data/V1_Breast_Cancer_Block_A_Section_1_image.tif'

#creates a raster object from the tiff image file
#imported_raster <- raster(str_name)
wsi <- brick(file_name)

#check to see the image contains the correct amount of pixels
dim(wsi)
plotRGB(wsi)

e <- extent(13091, 13341, 4639, 4889)
cropped_img <- crop(wsi, e)
plotRGB(cropped_img)
```


Defining a function to crop the spot images according to their centers
```{r}
# grabbing only the spots with tissue
image_location <- image_location %>%
  filter(in_tissue == 1)

#defining a function to get a cropped spot image given the x and y coordinates of the center of the spot
crop_spot <- function(img, x, y){
e <- extent(x - 136, #using 136 since it is ~half the average distance (273 pixels) between spot centers. We could consider using a different measure to more closely fit to the spot ratius.
              x + 136,
              y - 136,
              y + 136)
cropped_img <- crop(img, e)
return(cropped_img)
}
```


Partitioning the spots into testing and training data using a random sampling
(try implementing combine from here: https://www.youtube.com/watch?v=LxTDLEdetmI)
```{r}
set.seed(1998)
#nrow should be used when looking at the entire data set and no longer restricting to 500 spots
index <- sample(2,
                nrow(image_location),
                replace = TRUE,
                prob = c(0.7, 0.3)) #splits data into testing and training

training_data <- array(NA, dim=c(sum(index == 1), 272, 272, 3))
index_count <- 1 
training_indexes <- which(index == 1)
print("Collecting training data")
pb <- txtProgressBar(min = 0, max = length(training_indexes), style = 3)
for (i in training_indexes) #This for loop takes about an hour and a half when run on all spots
{
  cropped_img <- crop_spot(wsi, 
                           image_location$pxl_row_in_fullres_xValue[i], 
                           image_location$pxl_col_in_fullres_yValue[i])
  #Manipulate colors??
  #plotRGB(cropped_img) 
  cropped_img <- as.array(cropped_img)
  training_data[index_count,,,] <- cropped_img
  setTxtProgressBar(pb, index_count)
  index_count <- index_count + 1
}
close(pb)
#dim(training_data)


testing_data <- array(NA, dim=c(sum(index == 2), 272, 272, 3))
index_count <- 1
testing_indexes <- which(index == 2)
print("Collecting testing data")
pb <- txtProgressBar(min = 0, max = length(testing_indexes), style = 3)
for (i in testing_indexes) #This for loop takes about 40 mins when run on all spots
{
  cropped_img <- crop_spot(wsi, 
                           image_location$pxl_row_in_fullres_xValue[i], 
                           image_location$pxl_col_in_fullres_yValue[i])
  #plotRGB(cropped_img)
  cropped_img <- as.array(cropped_img)
  testing_data[index_count,,,] <- cropped_img
  setTxtProgressBar(pb, index_count)
  index_count <- index_count + 1
}
close(pb)
#dim(testing_data)
```


Loading in gene expression data and formatting the indexes to their corresponding spot image
```{r}
feature_matrix <- Load10X_Spatial(data.dir = "/Users/colten/Desktop/Perkins_Lab_ST/Identifying (usable) image spots/data",
                         filename = "V1_Breast_Cancer_Block_A_Section_1_filtered_feature_bc_matrix.h5",
                         assay = "Spatial",
                         slice = "slice1",
                         filter.matrix = TRUE,
                         to.upper = FALSE)

barcode_feature_matrix <- GetAssayData(object = feature_matrix, slot = "counts")
barcodes <- barcode_feature_matrix@Dimnames[2][[1]]
features <- barcode_feature_matrix@Dimnames[1][[1]]

indexes_of_training_barcodes <- match(image_location$barcode[training_indexes], barcodes)
training_barcode_feature_matrix <- t(barcode_feature_matrix[,indexes_of_training_barcodes])
dim(training_barcode_feature_matrix)

indexes_of_testing_barcodes <- match(image_location$barcode[testing_indexes], barcodes)
testing_barcode_feature_matrix <- t(barcode_feature_matrix[,indexes_of_testing_barcodes])
dim(testing_barcode_feature_matrix)
```






CNN testing
```{r}
index <- 1:10

par(mfcol = c(2,5), mar = rep(1, 4), oma = rep(0.2, 4))
training_data[index,,,] %>% 
  purrr::array_tree(1) %>%
  purrr::map(as.raster, max = 255) %>%
  purrr::iwalk(~{plot(.x)})

# Fix structure for 2d CNN
model <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "relu", 
                input_shape = c(272,272,3)) %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_flatten() %>% 
  layer_dense(units = 10, activation = "relu") %>% 
  layer_dense(units = ncol(training_barcode_feature_matrix), activation = "softmax")


summary(model)

#notes
#start simple to establish baseline
#fully linear: simplify to 1 pixel (RGB value so 3 inputs to network)
#don't need C in CNN just make a linear layer 3 inputs #genes outputs (use linear regression code for this)
#percentage of variance explained
#maybe average over pixel region?

model %>% compile(
  optimizer = "adam",
  loss = "categorical_crossentropy",
  metrics = "mae"
)
#model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
#t_array <- array(NA, dim = c(1, length(training_barcode_feature_matrix[,1]),length(features)))

#t_array <- matrix(as.numeric(as.vector(training_barcode_feature_matrix)), nrow = length(training_barcode_feature_matrix[,1]), ncol = length(features))[,1:10]

#t_array <- array_reshape(as.array(training_barcode_feature_matrix), c(length(training_barcode_feature_matrix[,1]), length(features)))

t_array <- as.matrix(data.frame(training_barcode_feature_matrix))
colnames(t_array) <- NULL
rownames(t_array) <- NULL

history <- model %>% 
  fit(
    x = training_data, y = t_array,
    batch_size = 64,
    epochs = 5
    #validation_split = 0.2,
    #verbose = 2
  )

  plot(history)

#evaluate(model, testing_data, as.matrix(testing_barcode_feature_matrix)[,1], verbose = 0)
#predict_classes(model, testing_data)
```



```{r}
library(caret)
set.seed(123)

n = 400
s = seq(.1, n / 10, .1)
x1 = s * sin(s / 50) - rnorm(n) * 5 
x2 = s * sin(s) + rnorm(n) * 10
x3 = s * sin(s / 100) + 2 + rnorm(n) * 10
y1 = x1 + x2 + x3 + 2 + rnorm(n) * 2
y2 = x1 + x2 / 2 - x3 - 4 - rnorm(n)
 
df = data.frame(x1, x2, x3, y1, y2)
 
plot(s, df$y1, ylim = c(min(df), max(df)), type = "l", col = "blue")
lines(s, df$y2, type = "l", col = "red")
lines(s, df$x1, type = "l", col = "green")
lines(s, df$x2, type = "l", col = "yellow")
lines(s, df$x3, type = "l", col = "gray")
 
indexes = createDataPartition(df$x1, p = .85, list = F)
train = df[indexes,]
test = df[-indexes,]

xtrain = as.matrix(data.frame(train$x1, train$x2, train$x3))
ytrain = as.matrix(data.frame(train$y1, train$y2))
xtest = as.matrix(data.frame(test$x1, test$x2, test$x3))
ytest = as.matrix(data.frame(test$y1[1:16], test$y2[1:16]))

in_dim = dim(xtrain)[2]
out_dim = dim(ytrain)[2]

model = keras_model_sequential() %>%
  layer_dense(units = 100, activation="relu", input_shape=in_dim) %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = out_dim, activation = "linear")

model %>% compile(
  loss = "mse",
  optimizer = "adam")

model %>% summary()

model %>% fit(xtrain, ytrain, epochs = 100, verbose = 0)
scores = model %>% evaluate(xtrain, ytrain, verbose = 0)
print(scores)
predict(model, xtest)

#devtools::install_github("andrie/deepviz")
#library(deepviz)
#library(magrittr)

#model %>% plot_model()
```
